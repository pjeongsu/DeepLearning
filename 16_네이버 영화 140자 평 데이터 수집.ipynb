{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 우클릭해서 프레임 소스 보기가 있으면 iframe으로 만든거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네이버 영화 140자 평 데이터 수집\n",
    "- 소스보기 했을 때 원하는 데이터가 없었다\n",
    "- iframe이 있는지 확인해 보세요\n",
    "- 여기에는 iframe이 있었습니다\n",
    "- iframe의 src 속성의 주소를 요청해서 원하는 데이터가 있는지 확인한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 프레임 소스보기를 해서 view-source:부분을 지우면 링크가 됨\n",
    "- id는 절대로 같은 값이 있을 수가 없음 -> 여기에서는 enumerate함수 사용\n",
    "- 데이터를 수집하는 과정에서 누군가가 리뷰를 작성할 수 있음 -> 중복 데이터 조심!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딕셔너리 넣고 저장하는 것은 맨 마지막 단계에 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10), (1, 20), (2, 30), (3, 40), (4, 50)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1 = enumerate([10, 20, 30, 40, 50])\n",
    "list(k1)\n",
    "\n",
    "# (인덱스, 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 함수\n",
    "def getSource(site) :\n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다\n",
    "    response = requests.get(site, headers = header_info)\n",
    "    # print(response.text)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 코드 수집 함수\n",
    "\n",
    "def getMovieCode():\n",
    "    site = 'https://movie.naver.com/movie/running/current.nhn'\n",
    "    \n",
    "    soup = getSource(site)\n",
    "    # print(soup)\n",
    "    \n",
    "    # ul 태그를 가져온다\n",
    "    a1 = soup.select_one('#content > div.article > div:nth-child(1) > div.lst_wrap > ul')\n",
    "    # print(a1)\n",
    "    \n",
    "    # li태그를 가져온다\n",
    "    a2 = a1.select('li')\n",
    "    # print(a2)\n",
    "    \n",
    "    # 코드들을 담을 리스트\n",
    "    code_list = []\n",
    "    \n",
    "    # li 태그의 수만큼 반복한다\n",
    "    for a3 in a2:\n",
    "        a4 = a3.select_one('dl > dt > a')\n",
    "        # print(a4)\n",
    "        \n",
    "        # href 속성값을 가져온다\n",
    "        href = a4.attrs['href']\n",
    "        # print(href)\n",
    "        \n",
    "        # =를 기준으로 문자열을 잘라내어 코드값만 가져온다\n",
    "        data1 = href.split('=')[1]\n",
    "        # print(data1)\n",
    "        \n",
    "        code_list.append(data1.strip())\n",
    "        \n",
    "    return code_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지의 데이터를 수집해 저장하는 함수\n",
    "def getData(soup, page):\n",
    "    \n",
    "    # 데이터를 담은 딕셔너리\n",
    "    data_dict = {\n",
    "        '평점' : [],\n",
    "        '리뷰' : [],\n",
    "        '작성자' : [],\n",
    "        '작성시간' : []\n",
    "    }\n",
    "    \n",
    "    # ul태그를 가져온다\n",
    "    a1 = soup.select_one('body > div > div > div.score_result > ul')\n",
    "    # print(a1)\n",
    "    \n",
    "    # li 태그들을 가져온다\n",
    "    a2 = a1.select('li')\n",
    "    # print(a2)\n",
    "    \n",
    "    # li 태그의 수만큼 반복한다\n",
    "    for idx, a3 in enumerate(a2):\n",
    "        \n",
    "        # 평점을 가져온다\n",
    "        a4 = a3.select_one('div.star_score > em')\n",
    "        data1 = a4.text.strip()\n",
    "        # print(data1)\n",
    "\n",
    "        # 리뷰를 가져온다\n",
    "        a5 = a3.select_one(f'#_filtered_ment_{idx}')\n",
    "        data2 = a5.text.strip()\n",
    "        # print(data2)\n",
    " \n",
    "        # 작성자를 가져온다\n",
    "        a6 = a3.select_one('div.score_reple > dl > dt > em:nth-child(1)')\n",
    "        data3 = a6.text.strip()\n",
    "        # print(data3)            \n",
    "            \n",
    "        # 작성일을 가져온다\n",
    "        a7 = a3.select_one('div.score_reple > dl > dt > em:nth-child(2)')\n",
    "        data4 = a7.text.strip()\n",
    "        # print(data4)          \n",
    "        \n",
    "        # print(data1)\n",
    "        # print(data2)\n",
    "        # print(data3)\n",
    "        # print(data4)\n",
    "\n",
    "        # 딕셔너리에 데이터를 담는다\n",
    "        data_dict['평점'].append(data1)\n",
    "        data_dict['리뷰'].append(data2)\n",
    "        data_dict['작성자'].append(data3)\n",
    "        data_dict['작성시간'].append(data4)\n",
    "\n",
    "    # 데이터 프레임 생성\n",
    "    df1 = pd.DataFrame(data_dict)\n",
    "    # display(df1)\n",
    "    \n",
    "    if os.path.exists('data3.csv') == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv('data3.csv', encoding='utf-8-sig', index=False) # encoding='utf-8-sig' : 엑셀로 열 때 한글이 안깨지도록\n",
    "    else:\n",
    "        df1.to_csv('data3.csv', encoding='utf-8-sig', index=False, header=False, mode='a') # mode='a' 기존의 것에 붙여줌\n",
    "    \n",
    "    # 전체 페이지의 수를 계산한다\n",
    "    a8 = soup.select_one('body > div > div > div.score_total > strong > em')\n",
    "    data5 = a8.text.strip()\n",
    "    # print(data5)\n",
    "    # 쉼표 제거\n",
    "    data5 = data5.replace(',', '')\n",
    "    # 전체 페이지 수를 계산한다\n",
    "    total_page = int(data5) // 10\n",
    "    if int(total_page) % 10 > 0:\n",
    "        total_page = total_page + 1\n",
    "    # print(total_page)\n",
    "    \n",
    "    # 테스트용 코드\n",
    "    total_page = 5\n",
    "    \n",
    "    if total_page > page:\n",
    "        # 전체 페이지 수가 현재 페이지 번호 보다 크면\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19335 : 5 수집중\n",
      "수집완료\n"
     ]
    }
   ],
   "source": [
    "# 영화 코드들을 가져오기\n",
    "movie_code_list = getMovieCode()\n",
    "\n",
    "# 테스트용 코드\n",
    "movie_code_list = movie_code_list[:3]\n",
    "\n",
    "for movie_code in movie_code_list:\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "        print(f'{movie_code} : {page} 수집중')\n",
    "        site = f'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code={movie_code}&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page={page}'\n",
    "        soup = getSource(site)\n",
    "        chk = getData(soup, page)\n",
    "\n",
    "        if chk == True:\n",
    "            page += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "print('수집완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
